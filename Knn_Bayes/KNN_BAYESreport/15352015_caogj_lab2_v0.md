<center><font style=font-size:32px>**中山大学移动信息工程学院本科生实验报告** </font></center>

<center><font style=font-size:20px>**（2017年秋季学期）**</font></center>

<div><div style="float:left;">课程名称:*Artificial Intelligence*</div>

|   年级   |      1501       |  专业（方向）   |       移动互联网       |
| :----: | :-------------: | :-------: | :---------------: |
| **学号** |  **15352015**   |  **姓名**   |      **曹广杰**      |
| **电话** | **13727022190** | **Email** | 1553118845@qq.com |

<font size=3>

[TOC]

</font>

###一、实验题目

<center> <font size=3>K近邻模型处理分类问题 </font> </center>

###二、实现内容

1. 使用KNN处理分类问题。在验证集上，通过调节K值、选择不同距离等方式得到一 个准确率最优的模型参数，并将该过程记录在实验报告中。
2. 在测试集上应用步骤1中得到的模型参数（K，距离类型等），将输出结果保存为 “学号_姓名拼音_KNN_classification.csv”，

####算法原理

&emsp;&emsp;KNN模型在应对大量的数据集的时候，只选择与测试数据相近的信息，缩小计算范围由此减少计算量——这种方式减少了与测试样本无关的数据对样本可能产生的影响。

&emsp;&emsp;KNN问题处理数据分类。KNN会在已有的诸多集合中通过**距离计算**，选择K个有价值的数据，**缩小计算的范围**。这里有价值的数据就称为近邻数据，这些数据将会影响到当前数据的分类，指导当前数据实现分类操作。

&emsp;&emsp;Knn处理数据回归。Knn会根据定义的距离计算方式获得与当前处理信息最贴近的数据，并根据已有的数据使用**特定的算法**获得每种情感的比例。

&emsp;&emsp;Knn方法的不足。Knn方法当然不是完美的，这种方法在消除了无关数据对数学模型的影响之后，同时放弃了其他有关模型或者关联性较弱的数据对于当前模型的加权意义。这就使得Knn方法可以在大致方向上与其同类的消息匹配得很好，但是在数据的调整上很少有发挥的余地。

* 距离的计算：距离的计算可能有多种方式。包括曼哈顿距离、欧式距离以及余弦距离等。所谓距离计算方式是对当前的分类操作建立一个数学模型，对于实际问题而言，不同的数学模型可能会有不同的效果，这也取决于我们的模型的构建以及数学关系的嵌套是否可以与实际问题符合得更好。
* 缩小计算范围。当笔者在分析当前的一条数据与数据集中的所有数据时，我们不敢保证我们的算法可以发现每一条信息隐藏的关系——这种情形下，如果一定要将与当前处理的数据关系微弱的信息纳入当前的数学模型，十有八九会使得模型为了迁就该部分数据而导致数学模型的波动与失衡。所以，Knn就设计模型以避开这个问题，Knn只选择关系最紧密的几个近邻值纳入数学模型，这就十分有效地避免了无谓数据对模型的影响。
* 特定的算法：与距离的计算很类似，需要根据具体的情况，结合实际问题，创建数学模型以解释当前的情况。

####伪代码

&emsp;&emsp;在实现数据处理之前，我们需要构建几个全局通用的数据库。

- 所有词的数据库，词之间互不重复。该数据库将用于构建one hot矩阵。
- 训练集每一行的信息。该数据库将用于在训练集巨大的数据量中筛选与当前数据最靠近的几个近邻值，用于作为遍历时候的索引。

#####整体流程

<center><img src="http://imgsrc.baidu.com/forum/pic/item/559e28381f30e92495963b5847086e061d95f73b.jpg" width=353 height=234 /></center>

####关键代码
&emsp;&emsp;为了处理数据的方便，笔者创建了一个新的类，专门用于储存train数据集中每一行的数据，并为之设立了必要的接口。

```c++
class Tuple{
public:
  // 消化分析一行的字符串，以更新成员变量；
	void Dismantle(string s, bool fix_base);//bool-是否添加入数据库中
	void Updaterow(); 
	void Register(string s){
        Dismantle(s, false);
        Updaterow();
    }
	double Distanceof(Tuple tuple);
	
	vector<double> row;//one hot的一行（在整体词集中，这一行的词是否出现过）
	vector<string> vca;//句子转化为词集
	string label;//标签
};
```

&emsp;&emsp;接下来的代码分析，笔者依据在“伪代码/整理流程”中的逻辑进行实际实现。

##### 处理训练集

```c++
while(逐个字符读入){
    if(字符 == '\n'){
      Tuple tp;
      tp.Dismantle(s_tuple, true); // 更新一行内的数据
      Tuplebase.push_back(tp); // 更新全局数据库
      s_tuple = ""; // 收集字符的容器字符串清零
    }else{
      s_tuple += c; // 收集当前的字符
    }
}
```

&emsp;&emsp;在处理训练集数据的时候，我们主要的任务还是构建用于后续计算的数据库，以及更新数据库。全局数据库是一个方面，另一个方面，在查询k个近邻值的时候，每一行都是一个单位，这些行都将会作为一个单位出现在比较距离的程序之中。完成了这个步骤，训练集的数据才真正对我们的后续计算有意义。

##### 当前数据分类

&emsp;&emsp;笔者在使用Knn对当前的一个句子进行情感分类的时候，按照要求，只依照一个条件——距离。所以笔者在后续的近邻值选择中，也就将问题简化为只有两个参量的问题：标签和距离。此二者的对应关系并非一一对应——这就意味着尽管我们需要其二者的映射关系，但是我们不能使用Map这种数据结构——在后续的比较中，又一定要遍历所有元组的标签和距离，从中选择距离最近的几个作为近邻值。为此，笔者为这两个参量的对应关系新建了一个结构体：

```c++
struct Candylabel{// candidate label-备选标签
	Candylabel(double dst, string lb){
		distance = dst;
		label = lb;
	}
	double distance;// 该标签值对应的距离
	string label; // 训练队列中的一个标签值
};
```

&emsp;&emsp;在遍历所有的Candylabel的时候，获得当前行的标签是非常容易的——标签正是笔者定义的一个成员变量，但是距离的运算就更为复杂了。这种情况下，笔者的选择是：

- 创建备选标签（标签映射距离）类的集合以备遍历
- 遍历一遍所有的训练集数据，以更新备选标签集合中元素的数据

```c++
vector<Candylabel> labelbase; // 创建备选标签的集合
// 遍历训练数据集，更新备选标签数据的内容
for(int i=0; i<Tuplebase.size(); i++){
  double crt_dst = tp.Distanceof(Tuplebase[i]);
  string crt_label = Tuplebase[i].label;
  Candylabel cl(crt_dst, crt_label);
  labelbase.push_back(cl);
}
```

##### 近邻值的选取

&emsp;&emsp;随着备选标签集合数据的逐步更新，备选标签集合也就逐渐变得可用。笔者也就可用从备选标签集合中选择最靠前的几个近邻值。

```c++
sort(labelbase.begin(), labelbase.end(), labelCmp);
// 按照定义排序，排序之后，近邻值就显现出来了
```

&emsp;&emsp;获得近邻值集合之后，我们只需要：

- 对近邻值集合进行遍历；
- 找到其中出现次数最多的标签作为预测标签；

```c++
#ifdef Most
map<string, int> labelnum;// 对于某种标签的计数；
int max_num = 0;
string best_label;
for(int i=0; i<k; i++){// 遍历近邻值集合；
  labelnum[labelbase[i].label] += 1;
  int cnt = labelnum[labelbase[i].label];
  if(cnt >= max_num){// 更新近邻值集合众数；
    max_num = cnt;
    best_label = labelbase[i].label;
  }
}// CatchIT
//	cout << "label:" << best_label << endl;
```

之后的工作就是对于一个validation文件中的所有数据采取同样的操作方式，就可以得到预测集合的信息。对预测信息的数据集与validation原有的数据集进行比较，就可以获知准确率了。

```c++
double Ratio(vector<string> va, vector<string> vb){
  // 两个vector分别表示预测集合与validation集合的情感标签信息
	double size = va.size(), cnt = 0;
	for(int i=0; i<size; i++){
      // 计数相同元素
		if(va[i] == vb[i])	cnt++;
	}
	printf("cnt=%lf\n", cnt);
	printf("size=%lf\n", size);
	return ((double)cnt/size);// 比例输出
}
```

####创新点与优化

&emsp;&emsp;使用对数数学模型，对近邻值的数据进行拟合，以求得到情感信息的判断优化。

笔者使用对数模型，对近邻值中的每一个数据都给予了一个为判断加权的机会。因为在近邻值的选取中，可能会出现以下情况：

- 近邻值大部分的情感是与测试数据不符的
- 少数情感文本中有一个与当前测试数据极为符合

按照众数决策，这时候一定会选择虽然不符合，但是大多数的答案，而笔者尝试对数模型去努力避开这一点。

```c++
//	calculating the posibilities;
// 根据近邻值使用对数数学模型，对不同的情感的权值进行计算；
	map<string, double> labelwgt;
	double sum_distance=0, sum_wgt=0;
	for(int i=0; i<n; i++)	sum_distance += labelbase[i].distance;
	for(int i=0; i<n; i++){
		labelwgt[labelbase[i].label] += abs(log( sum_distance/(labelbase[i].distance)) );
		sum_wgt += abs(log( sum_distance/(labelbase[i].distance)) );
	}
//	之后是对最高权值的情感进行输出
```

###三、实验结果与分析

1. 实验结果示例展示

   <img src="http://imgsrc.baidu.com/forum/pic/item/31b162d0f703918fe07c9aa35a3d269758eec444.jpg" width=443 height=243 />

2. 评测指标展示及分析

   当前的计算结果是基于：

   - 欧拉距离
   - 众数决策
   - 近邻值为13的情况下得到的准确率。

   这显然不是最大的准确度，笔者之前在调试的时候，最高的准确度达到40.836%，但是忘记了数据，由于时间紧迫，故没有将已经计算得出的数据展示出来，见谅。


##### 关于准确度的提升

&emsp;&emsp;准确度提升可以从几个方面入手：

- 距离的计算方式

  除了使用欧式距离之外，还可以使用曼哈顿距离，对不同的元组之间进行计算。目前测试的情况，曼哈顿距离的均值要更高一些——而且准确度也更加稳定，虽然都不能超过40%，但是从5到30这个近邻区间，准确度也都可以到达35%到39%之间。

  总之，曼哈顿距离：稳定

  而对于欧式距离来说，尽管在笔者的测试中，欧式距离可以达到40.836%的准确度，但是在近邻值小于10的时候，准确度会急剧下降。

#####测评指标展示：

- 附上记录表仅供参考：

  ```c++
  k=30	37.5&
  k=25	37.1795%
  k=19	36.5385%
  k=17	39.1026%
  k=16	38.4615%
  k=15	39.1026%
  k=14	38.4615%
  k=13	40.836%++
  k=12	39.1026%
  k=11	39.1026%
  k=9     33.9744%
  k=7     34.2949%
  ```

- 近邻值的数据处理

  在获取近邻值之后，按照要求应该是对近邻值数据进行众数决策，选择近邻值数据中出现测试最多的标签作为我们的预计标签。

  这种方式：

  - 实现了对于相似数据的符合。与当前测试文本相似的数据都可以对我们的预测起到很好的指导作用。
  - 在陌生的数据出现时，预测效果差。我们获知的数据很多都是与训练集数据没有交集的——所谓没有交集，就是没有任何一个词是重复的。这种情况下，对近邻值的选择更多的都是文本出现的顺序，以及与当前测试数据的词数更为相近的数据成为了近邻值——很显然这并不是一个合适的筛选方案。

  所以笔者在根据已有的基础上，实现了另一种计算方式，对近邻值中的数据，都给予加权的机会，使得这些数据都会为我们的决策产生影响——目前测试效果来看，该分类方法比较稳定，但是准确度还是没有超过40%。

  ---
###一、实验题目

<center>

 <font size=3>K近邻模型处回归问题 </font> 

</center>

###二、实现内容

1. 使用KNN处理回归问题，在验证集上，通过调节K值、选择不同距离等方式得到一个相关系数最优的模型参数，并将该过程记录在实验报告中。这一步可以通过使用 “ validation相关度评估.xlsx”文件辅助验证（也可以自己写代码）。
2. 在测试集上应用步骤1中得到的模型参数（K，距离类型等），将输出结果保存为 “学号_姓名拼音_KNN_regression.csv”。

####伪代码

&emsp;&emsp;在实现数据处理之前，我们需要构建几个全局通用的数据库。

- 所有词的数据库，词之间互不重复。该数据库将用于构建one hot矩阵。
- 训练集每一行的信息，元组集合。该数据库将用于在训练集巨大的数据量中筛选与当前数据最靠近的几个近邻值，用于作为遍历时候的索引。
- validation文件的句子信息集合。保存了validation文件中表达信息的句子，并以字符串格式保存在vector中。

#####整体流程

<center><img src="http://imgsrc.baidu.com/forum/pic/item/f9f2034f78f0f736a493e9680155b319eac413fb.jpg" width=353 height=234 />

</center>

笔者在分类的基础上，对原有的主函数内部的实现进行了打包与封装，这里流程图可以找到与之对应的伪代码，如下：

```c++
int main(int argc, char** argv) {
	train("train_set.csv"); // 分析训练数据集
	for(int i=0; i<元组个数; i++){ // 更新元组集合
		当前元组更新TF矩阵的一行；
	}

	string file_name="validation_set.csv";
	valid(file_name);// 分析validation文件
	for(int i=0; i<validation文件的数据数目; i++){
		由数据字符串注册新的元组tp；
		tp.taste(5);// tp元组尝出感情，更新感情成员变量
	}
	/*
	输出数据
	*/
	return 0;
}
```



####关键代码

#####结构体框架

&emsp;&emsp;正如在分类中所描述的，笔者为导入的数据的每一行都设计了类以拟合当前的信息，方便后续的操作。表示当前文件的每一行，这种传统在回归问题上得到了传承，但是又有所发展：

```c++
class Tuple{ // 元组：如同train_set的一行
  public:
  void Collectvoc(string s, bool fix_base);// 从s中收集词，bool值决定是否添加到库中
  void Updaterow(); // 实现更新TF矩阵的一行
  double Distanceof(Tuple tuple);
  void taste(int scope);//对当前元组的词库进行分析，品尝到底是什么情感，int为近邻域

  vector<bool> row;
  vector<string> vca; // 获得句子，拆分成词库
  Feel brain; // 表示人的6中情感，Feel为一个自定义的结构体
};
```

这其中的Feel是表示人的六欲的结构体：

```c++
class Feel{
public:
	void setfeel(vector<double> feelbase);
	vector<double> feelbar;
//	debug
	void visit_feelbar();
private:
	double anger;
	double disgust;
	double fear;
	double joy;
	double sad;
	double surprise;
};
```

笔者另为之添加了接口，所以内部的私有成员变量一般用不到，这里也就不做过多的解释了。

按照惯例，接下来按照整体流程的顺序，分析如何处理训练集并将训练集数据添加到数据库中的。

##### 处理训练集

笔者在这里处理训练集的时候，是依照原有的分类函数进行修改的，整体格局相近，但是由于读入的字符串更加繁杂琐碎，使得字符串处理的部分占据了很大的篇幅，这种修改的结果就是可读性不强，凌乱。笔者这里就不过多地贴码了，因为这一部分与主体思路偏离。

&emsp;&emsp;回归计算在这里另外实现了一个函数，这样的修改使得之后的实现更为清晰、简洁与流畅。回归函数为taste，在Tuple类中。

##### 近邻区域的选择

为了实现回归计算，首先要做的，还是对近邻域的选择。这次实现对于近邻域的计算要求更高，我们需要不仅仅是标签和距离，我们需要连同情感信息在内的元组的所有内容。那么这个问题就会变成：

Q:如何在元组集合中选择最接近当前数据的集合？

选项：

- 对当前的元组集合进行自定义排序：

  就算是使用快排——假如笔者会的话，计算复杂度至少要n*log(n)。

- 对当前集合进行遍历，获得符合条件的K个数据：

  一次遍历，更新容器，获得近邻值，计算复杂度为n。

这里笔者考虑的是对于与当前数据相差过多的元组，我们可以直接选择舍弃，而不使其参与任何的排序过程，这种方式可以节省下大量的计算资源与时间。

以下为对于第二种方式的实现：

```c++
map<double, int> findtp;// Tuple（元组）的索引号
vector<double> cdst;// candidate distance options
int update_cdst = 0;
for(int pin=0; pin<scope; pin++)	cdst.push_back(100);
// 对所有的元组进行遍历，查找到更为相近的元组
for(int tpin=0; tpin<Tuplebase.size(); tpin++){
  double dst = Distanceof(Tuplebase[tpin]);
  // 当前距离小于该容器内的最大距离
  if(dst < cdst[cdst.size()-1]){
    // 更新
    update_cdst++;
    cdst.push_back(dst); // 更新距离值
    findtp[dst] = tpin;  // 更新与之对应的索引号！！！
    sort(cdst.begin(), cdst.end());// 几十个数字的排序
    cdst.pop_back();	// 更新元素信息
    // 更新结束
  }
}
```

注意到初始化的时候，笔者将容器内的每一个数字都初始化为100了。这种情况下其实还是有可能出岔子的——比如，距离值都大于100，比如设定的K值甚至大于训练集容量。这种情况下，很可能多出一些莫名其妙的数据，这些数据将会给之后的debug工作带来不可预料的困难。

为此添加提示监听：

```c++
if(update_cdst < scope){
	printf("[Tuple.cpp/taste]:Scope or distance is too big.\n");
	printf("[Tuple.cpp/taste]:Program abort.\n");
	exit(0);
}//这样就万无一失啦哈哈哈
```

至此，我们就获得了我们需要的几个元组的序号，但是并没有提取出它们的信息。不过找到它们也只是探囊取物。因为我们把每一个距离的信息都与遍历时候元组的信息进行了映射，而且在不断地更新。其名字就是findtp。

在拥有了近邻域之后，就是对回归操作的计算了。

```c++
vector<double> fb; // 容量为6，储存6种情感值
for(int i=0; i<6; i++){
  double fp = 0; // 表示当前的情感值，之后会收集到容器内
  // 近邻域遍历
  for(int dstpin=0; dstpin<cdst.size(); dstpin++){
    double dst=cdst[dstpin];
    int tpin = findtp[dst];
    // 若与近邻域中的某一个元组完全符合
    if(Distanceof(Tuplebase[tpin]) == 0){
      fp = 0;
      fp += Tuplebase[tpin].brain.feelbar[i];
      break;
    }else{
     // 若不完全符合
      double key_feel = Tuplebase[tpin].brain.feelbar[i];
      fp = fp + key_feel/(0.01+Distanceof(Tuplebase[tpin]));// 公式
    }
  }
  // 循环外，收集集合参数
  fb.push_back(fp);
```

在获得了6种情感值之后，将这个容器归一化后的信息添加到当前数据的成员变量中即可。

```c++
brain.setfeel(fb);
```

####创新点与优化

#####计算效率优化

很遗憾笔者在准确度上一直没有什么大的进展，但是笔者在运行效率上的修改是卓见成效的。从几十秒到20秒到十几秒。通过对无效计算的规避，消减了计算的复杂度。

#####debug处理

&emsp;&emsp;在编程过程中对可能发生的错误的预知可以在极大程度上减少后续修改中的困难。尤其在引入新的工具之后。笔者为减少排序的复杂度与计算量引入了vector，同时也需要对vector的困难引发的问题进行考虑。

##### 情感值的计算

&emsp;&emsp;有一个不变的问题就是数学模型的套用，在本次实验中，笔者使用的数学模型是双曲线。当然，双曲线在趋势上的把握还是准确的。但是在应对较小值与极小值的时候，双曲线就显得犹豫不决优柔寡断了。

&emsp;&emsp;如我们所知，在validation集合中，有很多数值为0，这些数值在双曲线中表现为似有似无的极小值，可能精确到小数点后几位，尽管在一个数据中影响不明显，但是在数据量高达几百个甚至几千个的时候，这种影响的叠加就不可控了——它可能会相互抵消，也可能再次叠加。但是不可控的算法也就不可信。为此，笔者做了以下改进：

```c++
double key_feel = Tuplebase[tpin].brain.feelbar[i];
if(key_feel > 0.5){
  key_feel *= 1.5;
  key_feel = (int)(100*key_feel);
  key_feel /= 100;
}else if(key_feel < 0.5){
  key_feel *= 0.3;
  key_feel = (int)(100*key_feel);
  key_feel /= 100;
}
fp = fp + key_feel/(0.01+Distanceof(Tuplebase[tpin]));
```

笔者在原有公式的基础上，添加了阈值的限定——之前讨论过的双曲线收敛速度过低的问题，在这里由笔者手动解决。尽管这种方法并没有逃离小垃圾的噩梦，但是在一定程度上进行了修正。

而与之类似的，还可以使用对数收敛（对数在0-1区间收敛速度飞快的）、指数收敛（和对数一样的）和截断收敛等等，不一而足。

与此类似的还有使用TFIDF矩阵，断定在某一个文本中的最重要的词——个人感觉在本次实验中意义不大，实验数据量少暂且不谈，内容出现频率也非常有限，TFIDF矩阵的战场并不多。

###三、实验结果与分析

####实验结果示例展示

<img src="http://imgsrc.baidu.com/forum/pic/item/d942b31c8701a18ba14bb526952f07082938fe49.jpg" />

####评测指标展示及分析

<img src="http://imgsrc.baidu.com/forum/pic/item/ec9f6b63f6246b605530141de0f81a4c510fa237.jpg" />

对于笔者优化后的输出结果，相对于原本的信息都是”小辣鸡“的级别，我真是不想再说什么了。

测评指标以evaluation为准，二者相差不大，分析原因：

- 使用双曲线的数学模型符合度不高，测试数据与比较元组的距离不足以限定存在的情感系数。

- 传入的参数有限：

  - 在当前测试数据中可能会出现训练集中都未曾出现过的数据，这时候参考数据不再具有参考性

  - 距离作为衡量的唯一标准这样有失公正，对于不同的句子来说，不一样的词语甚至决定了该句子的情感系数的可能性——如果句子中出现了词语“die”其负面情绪的可能性就大大增加，除非是坏人死了——怎么会有绝对的坏人呢？

    在实际的处理中，应该找到这种关键词，并为之分离形成一个更有优势的数据库，再结合当前的距离信息，综合对当前数据进行考虑。

    distance仅仅表示相似度，用它作为衡量的唯一标准就已经犯了方向性的错误，这已经不再是凑几个模型或者尝试几次排列能够修正的了。

---
###一、实验题目

<center> <font size=4>Classification for Naive Bayes</font> </center>

###二、实现内容

1. 使用Bayes处理分类问题，只要求实现多项式方法。
2. 使用Bayes处理回归问题，使用相关系数衡量实验结果。

####算法原理

&emsp;&emsp;Bayes方法是对于同样的事件，通过先验概率推测后验概率的方法。分为伯努利模型与多项式模型，本次实验中实现多项式模型。

&emsp;&emsp;在多项式模型中，遵循一个假设——即所有词的出现都是各自独立的，不会因为出现了“Same”前面就一定要加一个定冠词。这种情况下，对于每一个词出现的概率就可以估算了，本次实验中使用频率代替概率。

&emsp;&emsp;所以当一个句子中出现了很多词，每一个词在出现的概率都各自独立，既然这些词恰好组成了当前的句子，就应该把这些词出现的概率相乘。而对于不同的情感来说，每一个词在不同的情感中出现的概率都不太一样，这样计算出的不同情感的权值也就不太一样。

&emsp;&emsp;通过这些概率，我们可以在后续处理无论是分类问题还是回归问题中都有所依仗。

####伪代码

Bayes的伪代码与之前Knn的伪代码非常类似，都是对相关数据的导入与关联函数的调用。整体流程如下：

```c++
int main(int argc, char** argv) {
  // 读入训练数据并分析得到数据库
  // 根据数据库内容，计算更新训练数据内容

  valid("validation文件名");
  for(遍历validation的句子){
    // 储存为元组格式
    // 分析label
    // 拟合为输出容器
  }
	
	fout << "id" << ',' << "label" << endl;
	for(遍历输出文件){
		fout << i+1 << ',' << "获得的label" << endl;
	}
	// 对输出的文件内容与validation的文件内容进行分析
  	// 计算准确度
	return 0;
}
```

####关键代码

在笔者实现本次实验之前，需要设计全局变量，作为贯穿整个project的数据库，这些数据将会作为比较和计算的标准：

1. 所有词的有序集合（以出现顺序为序）
2. 元组的集合，即训练集本身
3. validation文件的句子的集合，将会作为处理validation中每一个文本时候的遍历索引

#####结构体框架

由于处理文本的缘故，本次实验中笔者依然使用了Tuple作为储存和管理一个文件的一行的信息的数据结构：

```c++
class Tuple{
public:
  	void Collectvoc(string s, bool fix_base); // 拆词入库， bool 决定是否入库
	void Updatebaserow(); // 更新 row
	void Register(string s){
        Collective(string s, bool fix_base);
      	Updaterow();
    }
	double Distanceof(Tuple tuple);
	void taste(); // 由 voca 推算 label
	vector<bool> row; // 在所有词出现的顺序集合中，该行中的词所占的位置
	vector<string> voca; // 该行中出现的词
	string label; // 标签
};
```

在拥有了这个数据结构之后，所有的文件通过特定的函数调用不断地满足以上的变量信息，就可以进行后续的操作。

&emsp;&emsp;笔者在这里处理训练集的时候，是依照原有的分类函数进行修改的，整体格局相近，但是由于读入的字符串更加繁杂琐碎，使得字符串处理的部分占据了很大的篇幅，这种修改的结果就是可读性不强，凌乱。笔者这里就不过多地贴码了，因为这一部分与主体思路偏离。读取文件之后，需要对当前需要识别的信息进行注册和分类。

&emsp;&emsp;在实现分类的时候，Bayes的处理方法是对当前数据的每一个情感都进行权值的计算，取最大值作为当前数据的情感。由于在实现的过程中，测试数据总看你出现训练数据中未曾出现的数据，这就直接导致输出信息为0，为此实验要求使用Laplace平滑对数据进行加工。

##### 对当前数据的情感分析

​	按照我们之前所说的，当前的情感信息取决于每一个词出现的概率相乘。这时候，对于某一个词出现的概率的计算就很有发挥的余地了，毕竟我们是以频率估算概率，这种估算方法总是花样繁多。在本次实验中，经过Laplace平滑的修正，最后的计算公式如下：

> 词A出现的概率 = （A在感情a中出现的频率+1）/（感情a的词数+总词数）[^1]

为了获得了每一个词在不同的情感中的概率，笔者按照文档上给出的信息进行编码，得到逻辑如下：

```c++
int numofv = count_words(crt_em, crt_v);
// 情感crt_em的词库量-含重复词
int rptcnt = get_size(crt_em, true);
double tmpp = (double)(numofv+1)/(rptcnt + vocbase.size());//vocbase是全局变量
```

这种计算将会使得我们得到：

- 在当前测试句子内
- 每一个词的权值的乘积

在后续的实现中，只需要将不同情感下，这些词的乘积值进行比较，就可以获得权值最大的情感标签了。然而，为了获得以上的几个计数，笔者还是要进行一些函数的实现，包括count_words和get_size。

##### count_words函数的实现

- 顾名思义，该函数用于计算情感下该词的出现次数。
- 有两个参数：情感（字符串），被查询的词（字符串）;

```c++
if(emotion == "joy"){// emotion 为传入的参数，表示要在哪个数据库内查询
  if(joylabel_.size() == 0)	return 0;// joylabel_表数据库，为vector，repeatable
  int cnt = count(joylabel_.begin(), joylabel_.end(), word);// STL计数count
  return cnt;
}//其他情感以此类推
```

##### get_size函数的实现

- 意义：查询当前词库的词量
- 参数：感情（字符串），是否含重复词（bool）

```c++
if(emotion == "joy"){// emotion 和 repeat 分别为传入参量
  if(repeat)	return joylabel_.size();
  else	return joylabel.size();
}
```

有了这些函数的帮助，我们就可以实现在公式中给出的那些信息了。于是也就知道了不同情感下，每一个词的权值，对每一种情感下的词的权值进行乘积，结果正是：

<center>**在这种情感下，出现这句话的概率**</center>

再选择一个出现概率最大的情况，即为解。

####创新点与优化

总的来说，本次实验的实验要求中规中矩。按照要求实现的代码在最后都可以得到类似程度的效果。对于优化而言，由于分子使用的是在当前情感环境下对某一个词的计数信息，这个词出现的次数，会对我们计算的结果添加权值信息。而分母中的课重复词也在一定程度上抵消了这种加权的作用。这种操作使得计算的结果不再受训练集中某种情感的数量的影响。

可以优化的地方，是对于不同词的区别加权，对于介词和形容词的区别对待——然而笔者还没有实现。但是笔者放弃了在实验要求中，使用可重复集数量与非重复集数量的和的部分。改为使用全局的词库数量，使得计算值更为准确。

###三、实验结果与分析

####实验结果示例展示

<img src="http://imgsrc.baidu.com/forum/pic/item/e3a333fa828ba61e6c766c464a34970a314e5951.jpg" />

####评测指标展示及分析

测评指标为validation的符合度，这里显示为36.9775%。

这个符合度是基于训练集的所有信息的计算结果，在达到了Knn计算的平均水平的基础上，并没有给我们带来什么惊喜。

- 分析原因：

  尽管当前算法对于所有的词都进行了一定程度的拟合，产生结果也差强人意。但是很大程度上，并没有逃离相似度比较的模式。

  - 对于每一个词都同等看待；
  - 计算其出现的频率

  这两点，本身就是在延续使用相似度判决的方法，所以出现Knn的平均水平也就不足为奇，甚至，Knn根据近邻值域的选择，还使得筛选更具有不确定性【笑】，反而误打误撞地提升了准确度。

---
###一、实验题目

<center> <font size=5>Bayes for Regression</font> </center>

###二、实现内容

&emsp;&emsp;使用Bayes处理回归问题，使用相关系数衡量回归结果。

####算法原理

&emsp;&emsp;Bayes方法是对于同样的事件，通过先验概率推测后验概率的方法。分为伯努利模型与多项式模型，本次实验中实现多项式模型。

&emsp;&emsp;在多项式模型中，遵循一个假设——即所有词的出现都是各自独立的，不会因为出现了“Same”前面就一定要加一个定冠词。这种情况下，对于每一个词出现的概率就可以估算了，本次实验中使用频率代替概率。

&emsp;&emsp;所以当一个句子中出现了很多词，每一个词在出现的概率都各自独立，既然这些词恰好组成了当前的句子，就应该把这些词出现的概率相乘。而对于不同的情感来说，每一个词在不同的情感中出现的概率都不太一样，这样计算出的不同情感的权值也就不太一样。如此这般，也就知道所给出的数据在特定的感情下所出现的概率。

&emsp;&emsp;然而，这个概率的前提是感情是明确的，事实上，这些感情信息一直都作为系数存在，感情的系数也是不明确的。为了获得当前测试句子的最大概率，我们应该在已有的词汇概率的基础上，乘以当前的感情可能出现的概率——这才是在该感情下，当前句子可能出现的概率。

&emsp;&emsp;通过这些概率，我们可以在后续处理无论是分类问题还是回归问题中都有所依仗。

####伪代码

通过两个文件对算法的符合度进行进行比较。实验中给定了训练集数据与validation数据，笔者的实现步骤基于二者的数据，并对数据进行整合和运算。

#####整体思路

- 整合训练数据
- 对validation数据进行回归运算
- 输出运算结果

```c++
int main(int argc, char** argv) {
	train("训练集文本名称");// 自定义函数train
	for(int i=0; i<训练集的行数; i++){
		// 更新每一行的计算结果
	}

	valid("测试集数据文件名");// 自定义函数validation
	for(int i=0; i<测试集数据行数; i++){
      // 将测试集数据的一行，转化为一个Tuple
      // 根据测试集的数据计算情感系数
	}
	// 将得到的信息输出到CSV文本中
	return 0;
}
```

####关键代码截图

在笔者阅读了实验要求之后，进行实验之前，笔者意识到应该建立几个全局变量，这些全局变量将会在多处计算中派上用场：

- 所有出现过的词的有序集合（以出现顺序为序）
- 所有的元组的集合（实际上就是train本身）
- 所有的测试数据的文本的集合

```c++
extern vector<string> vocbase;
extern vector<Tuple> Tuplebase;
extern vector<string> Validbase;
```

##### 结构体框架

笔者在原有的元组基础上，为了建立新的情感数据管理机制，对该类进行了发展：

```c++
class Tuple{
public:
//	For updating; 部分接口
	void Collectvoc(string s, bool fix_base);// 拆词，更新voca数据, bool 表是否入库
	void Updatebaserow(); // 更新baserow数据
	double Distanceof(Tuple tuple);
	void taste(); // 由VOCA推算情感系数
	
	vector<string> voca; // 所有句子中的词
	vector<double> baserow; // 全局TF矩阵的一行
	vector<double> vocarow; // 与voca中的词对应的TF矩阵中的系数
private:
	vector<double> emotionbar; // 储存六欲的容器
};
```

&emsp;&emsp;目前已经将输入的数据转化为了储存的元组信息，这些信息将可以用于判断当前处理的数据的情感系数。

​	根据实验要求和Bayes的理论，当一个句子中出现了很多词，每一个词在出现的概率都各自独立，既然这些词恰好组成了当前的句子，就应该把这些词出现的概率相乘。而对于不同的情感来说，每一个词在不同的情感中出现的概率都不太一样，这样计算出的不同情感的权值也就不太一样。如此这般，也就知道所给出的数据在特定的感情下所出现的概率。

#####测试数据某词的概率估算

由于在实现的过程中，测试数据总可能出现训练数据中未曾出现的数据，这就直接导致输出信息为0，为此实验要求使用Laplace平滑对数据进行加工，即：

> 词A出现的概率 = (A在句子S中出现的次数+1)/(句子S的总词数 + K);

代码如下：

```c++
// 遍历句子中出现的所有的词
for(int vwpin=0; vwpin<vwgts.size(); vwpin++){
  // vwgts-value's weight 表示每一个词出现的次数/频率
  double iniwgt = vwgts[vwpin];
  // vwgtsum 是总词数	vwgts.size表示当前句子有多少个词
  vwgts[vwpin] = (iniwgt+1)/(vwgtsum + vwgts.size());
}
```

经过Laplace平滑的计算，我们可以得到这个词在这一行中出现的频率。接下来就是对其他的词也使用同样的算法，获得频率表示概率再相乘。

#####与当前情感概率相乘

​	然而，这个概率的前提是感情是明确的，事实上，这些感情信息一直都作为系数存在，感情的系数也是不明确的。为了获得当前测试句子的最大概率，我们应该在已有的词汇概率的基础上，乘以当前的感情可能出现的概率：

> 句子的概率 = 
>
> 词A在感情e下出现的概率 x 词B在感情e下出现的概率 x......x 最后一个词在感情e下出现的概率 x 感情e出现的概率

这才是在感情e的条件下，当前句子可能出现的概率：

```c++
if(crt_tp.emotionbar[empin] == 0){ // 若该行当前情感系数为0
  crt_em = 0;// 之前的一切计算作废
}else{
  // vwmultp 表示之前的（词的概率）的（乘积）
  crt_em = vwmultp * crt_tp.emotionbar[empin];
}
```

这样，我们就可以完成对一行的信息的比较了，测试集中还有很多数据，这些数据都需要进行比较，并将每一行都得到的概率做和。

```c++
for(int tpin=0; tpin<Tuplebase.size(); tpin++){
	...
  emoj[empin] += (10000*crt_em);
}
```

至此，不同的感情系数对预测的影响才算是计算结束。目前的emoj数据已经是对训练集数据的所有因素都考虑过计算过的值的归纳了。之后的工作就是对这个数组中的数据进行归一化、更新当前数据的情感系数。

####创新点与优化

笔者进行优化是基于Laplace平滑的基础上进行的优化操作。分析如下：

在Laplace平滑之前：

```c++
	double iniwgt = vwgts[vwpin];
    vwgts[vwpin] = iniwgt;
```

而在Laplace平滑之后：

> vwgts[vwpin] = (iniwgt+1)/(vwgtsum+vwgts.size());

所得出的数据是在原有的权值上除以一个大数进行消减，再为之添加一个常量。由分析可知，这种方式对于当前权值的波动影响有巨大的削弱。这种削弱将会掩盖真实的波动情况，为此，笔者修改当前的数学模型，以放大当前权值的影响。

而在对于数学模型的选择上，使用正比例函数的扩大是不合理的，因为之后的归一化函数将会对所有已经放大过同等倍数的数据除以这些数据的和——因为6种情感的emotion值都是放大了相同的倍数，所以归一化的时候我们所做的乘积操作就会毁于一旦。

除了正比例函数，还是有很多的函数可以用于放大差距，以区分当前数据的波动的，但是我们还要在这些函数中进行筛选，这种情况下我们就需要考虑我们正在处理的数据的特点：

- 处理的数据是词在当前的句子中出现的频率
- 频率处于0-1之间

为了在0-1之间实现大的区分度，可以使用一个有渐近线的函数，且渐近线为直线x = 1，但是这个函数有可能会造成数据的过度区分，这与我们预期的平滑目的相违背。因此，我们需要选择的模型：

- 在0-1区间内，没有过分的攀升
- 也不会造成原有的0的状况
- 同时还要避免常数项的覆盖影响。

所以，笔者使用指数函数在0-1部分的走向进行拟合:

> vwgts[vwpin] = pow(1.001, iniwgt) - 1 + Tiny;

得出结果：

<center><img src="http://img.bbs.csdn.net/upload/201710/18/1508288102_931873.jpg" /></center>

###三、实验结果与分析

1. 实验结果示例展示

   <img src="http://imgsrc.baidu.com/forum/pic/item/67bcc65c103853438e1c23489813b07ecb80881a.jpg" />

   优化后：

   <img src="http://imgsrc.baidu.com/forum/pic/item/67bcc65c103853438e1c23489813b07ecb80881a.jpg" />

2. 评测指标展示及分析

   嗯，又是小辣鸡……为什么会是小辣鸡呢？首先我们分析一下什么是小辣鸡——

   考虑我们的输出数据全部是0的情况，假如不考虑被除数为0，那么可以说我们的输出数据与处理的数据毫无关系。为了简化模型，笔者将这种情况考虑为0——其实并不是。

   那么小辣鸡就是在原有的毫无关联的基础上有一点正向的波动，但是波动的程度不够，换句话说，就是每一行的数据都差不多，或者——都差不多地和预期的数据没什么关系。

   笔者认为有如下几个原因：

   - 训练集中未出现的测试词集过多

     在尚未实现Laplace的情况下，如果当前的词在训练集中没有出现过，那么该词所占的比重就一定是0了，这种情况使得输出数据有一半都是0——就没有什么参考价值。而在实现了Laplace平滑的操作之后，所有本应该是0的数据都转变为一个平滑了的数据，虽然不是常数，但是也和常数差不多，因为训练集没有这个词的出现，所以就没有参考价值。这种情况使得本来半篇是0的情况转变为了半篇都是常熟的情况，本质上并没有发生过多的变化。

   - 数学模型中用于平滑处理的常数项所占的比例过大

     在尚未使用Laplace平滑之前，如果对输出的结果进行归一化，由于有很多的信息都是0，导致归一化之后的数据情感分明特别明显，或者都是1，或者是0.80以上，当然这未必符合我们的预期值。但是Laplace平滑之后，由于分子分母添加的常数值，如果遇到分母极小的情况，很可能会使得有微小波动的分子数据的波动被隐藏在小数点后4位到5位，这同样会使得数据的波动程度不够。
####四、思考

- 为什么Knn相似度加权的时候，要将距离的倒数作为权重

  距离表示当前检查的元组与比较的元组之间的距离，距离越长则这两个元组的相似度越低，对照的元组参考性就越低，这种情况下，参考性与距离之间就呈负相关关系。因而，使用反比例函数是符合要求的。

- 如何归一化

  如果当前的情感系数不都为0，那么可以对所有的系数做和，再将每一个系数除以系数之和，以达到归一化的目的。

  如果当前的情感系数都未被探测到，可以都设置为1/6，但是这种方法的对该问题的符合度不高。

- 矩阵稀疏度不同的时候，曼哈顿距离与欧式距离有什么不同

  曼哈顿距离与欧式距离都是距离范式中的一种形式，曼哈顿距离取参数值为1，欧式距离取参数为2。

  当矩阵非常稀疏的时候，进行计算的数组中都具有很多的0，而非零的数据密度极低。在计算距离的时候，如果非零的值大于1，则曼哈顿距离计算得到的距离更大，而值小于1的时候，计算的距离更小——所谓波动都比欧式距离更小，因此，欧式距离的计算结构更加稳定。

  结合矩阵的稀疏度，对于不同的矩阵：

  - 使用曼哈顿距离计算得到的数据波动幅度更大，可以作为区分矩阵的特征量
  - 使用欧式距离计算得到的数据幅度波动较小，可以限定对数据的过度拟合分析

- 伯努利模型与多项式模型各有什么优缺点

  伯努利模型的重点集中于文本的数量，伯努利模型是以文本为计算单位的。而多项式模型是以词为单位的。

  伯努利模型在应对区分度大的文本集的时候，优势大于多项式模型，伯努利模型的处理方式决定了其能够很清楚地分清，当前这个词，在文本中到底有多大的价值，如果在此类文本中，出现这个词的概率过于小，那就说明该词并不能被人们接受地表达该类别文本的意思。但是伯努利模型在处理少量数据的时候就显现出巨大的劣势——如果我们的文本数量少得可怜，但是文本的内部有具有大量的信息可以挖掘，伯努利模型在这方面的表现很显然是不足的。

  多项式模型对文本之间的区分并不是非常敏感，多项式模型在分析一个词的时候，参考的主体是整个词库，这个词库表达的内容繁多而且值得挖掘，如此，对于这个词的理解就很深刻。但是同时多项式模型淡化了文本的概念，过度放大了出现次数所占的比重，这种特点的结果就是对介词副词等无实际意义的连接词没有任何抵抗力，非常容易混淆视听。

- 如果测试集中出现了一个全词典都没有出现的词应该如何处理

  常规来讲我们是没有任何办法的，在Bayes的regression实验中，有大量的测试数据都没有在训练集中出现过，这种情况下，就不得不放弃对于整个句子的句意的猜测。

  如果该句子中还有其他的词汇，我们可以降低这个未知的词在分析过程中所占的比重，将重点转移到其他已知内涵的词语中。

  但是正如Bayes的regression实验中，有的句子中一个词汇都没有出现过，这种情况下就真的是一筹莫展，就算是人，也鲜有办法。